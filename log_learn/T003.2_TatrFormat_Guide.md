# T003.2 - TATR Data Preparation (COCO Format) - Learning Guide

**Subtask**: 3.2 - Implement TATR data preparation (COCO format, normalized boxes)
**Date**: 2025-12-04
**Audience**: Developers preparing data for table detection models

---

## What Was Built

A data preparation pipeline that converts synthetic invoice PDFs and ground truth labels into COCO format, suitable for training TATR (Table Transformer) and similar object detection models.

---

## What is COCO Format?

COCO (Common Objects in Context) is a standard format for object detection datasets:

```json
{
  "images": [
    {"id": 1, "file_name": "image.png", "width": 800, "height": 600}
  ],
  "annotations": [
    {"id": 1, "image_id": 1, "category_id": 1, "bbox": [x, y, w, h], "area": 1234}
  ],
  "categories": [
    {"id": 1, "name": "table"},
    {"id": 2, "name": "table_row"}
  ]
}
```

Key points:
- **images**: List of images with IDs and dimensions
- **annotations**: Objects detected, with bounding boxes
- **categories**: Classes of objects to detect

---

## What is TATR?

TATR (Table Transformer) is a DETR-based model for detecting tables in documents:

- Detects table boundaries
- Identifies table rows/columns
- Works with document images

Our COCO format prepares data for training such models.

---

## Bounding Box Normalization

### Why Normalize?

Different images have different sizes. Normalization ensures consistency:

```python
def normalize_bbox(bbox, width, height):
    return [
        int(bbox[0] * 1000 / width),   # x
        int(bbox[1] * 1000 / height),  # y
        int(bbox[2] * 1000 / width),   # width
        int(bbox[3] * 1000 / height)   # height
    ]
```

### Example

```
Original image: 612 x 792 pixels (Letter size PDF)
Original bbox:  [100, 200, 400, 50]

Normalized bbox:
  x: 100 * 1000 / 612 = 163
  y: 200 * 1000 / 792 = 252
  w: 400 * 1000 / 612 = 653
  h: 50 * 1000 / 792 = 63

Result: [163, 252, 653, 63]
```

Now all bboxes are in 0-1000 range regardless of image size.

---

## PDF to Image Conversion

We use `pdf2image` (wrapper around poppler):

```python
from pdf2image import convert_from_path

def pdf_to_image(pdf_path, dpi=150):
    images = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)
    img = images[0]
    if img.mode != 'RGB':
        img = img.convert('RGB')
    return img
```

Key parameters:
- **dpi=150**: Balance between quality and file size
- **first_page=1, last_page=1**: Only convert first page
- **RGB conversion**: Ensures consistent color mode

---

## Extracting Table Bounding Boxes

### Table BBox

The table bbox encompasses all line items:

```python
def extract_table_bbox(ground_truth):
    line_items = ground_truth.get('line_items', [])

    # Get all valid bboxes
    valid_bboxes = [item['bbox'] for item in line_items if item.get('bbox')]

    if not valid_bboxes:
        return None

    # Calculate encompassing bbox
    min_x = min(b['x'] for b in valid_bboxes)
    min_y = min(b['y'] for b in valid_bboxes)
    max_x = max(b['x'] + b['width'] for b in valid_bboxes)
    max_y = max(b['y'] + b['height'] for b in valid_bboxes)

    return [min_x, min_y, max_x - min_x, max_y - min_y]
```

### Row BBoxes

Each line item becomes a table_row annotation:

```python
def extract_row_bboxes(ground_truth):
    row_bboxes = []
    for item in ground_truth.get('line_items', []):
        bbox = item.get('bbox')
        if bbox:
            row_bboxes.append([bbox['x'], bbox['y'], bbox['width'], bbox['height']])
    return row_bboxes
```

---

## Building COCO Dataset

### Step 1: Create Empty Dataset

```python
def create_coco_dataset():
    return {
        'images': [],
        'annotations': [],
        'categories': [
            {'id': 1, 'name': 'table', 'supercategory': 'document'},
            {'id': 2, 'name': 'table_row', 'supercategory': 'table'}
        ]
    }
```

### Step 2: Add Images

```python
def add_image_to_coco(dataset, image_id, file_name, width, height):
    dataset['images'].append({
        'id': image_id,
        'file_name': file_name,
        'width': width,
        'height': height
    })
```

### Step 3: Add Annotations

```python
def add_annotation_to_coco(dataset, annotation_id, image_id, category_id, bbox):
    area = bbox[2] * bbox[3]  # width * height

    dataset['annotations'].append({
        'id': annotation_id,
        'image_id': image_id,
        'category_id': category_id,
        'bbox': bbox,
        'area': area,
        'iscrowd': 0
    })
```

---

## Full Pipeline

```python
def format_tatr(input_dir, output_dir, seed=42):
    # 1. Create output directories
    tatr_output = os.path.join(output_dir, 'tatr')
    images_output = os.path.join(tatr_output, 'images')

    # 2. Initialize COCO dataset
    coco_dataset = create_coco_dataset()

    # 3. Process each invoice
    for idx, label_file in enumerate(label_files):
        # Load ground truth
        with open(label_path) as f:
            ground_truth = json.load(f)

        # Convert PDF to image
        image = pdf_to_image(pdf_path)
        image.save(os.path.join(images_output, f'{base_name}.png'))

        # Add to COCO dataset
        add_image_to_coco(coco_dataset, idx+1, f'{base_name}.png', w, h)

        # Add table annotation
        table_bbox = extract_table_bbox(ground_truth)
        if table_bbox:
            normalized = normalize_bbox(table_bbox, w, h)
            add_annotation_to_coco(coco_dataset, ann_id, idx+1, 1, normalized)

        # Add row annotations
        for row_bbox in extract_row_bboxes(ground_truth):
            normalized = normalize_bbox(row_bbox, w, h)
            add_annotation_to_coco(coco_dataset, ann_id, idx+1, 2, normalized)

    # 4. Save annotations
    with open(os.path.join(tatr_output, 'annotations.json'), 'w') as f:
        json.dump(coco_dataset, f, indent=2)
```

---

## Output Structure

```
data/formatted/tatr/
├── images/
│   ├── invoice_00000.png
│   ├── invoice_00001.png
│   └── ...
└── annotations.json
```

---

## Using with TATR/DETR

After generating COCO format, train with:

```python
from transformers import DetrForObjectDetection
from datasets import load_dataset

# Load our COCO dataset
dataset = load_dataset('json', data_files='annotations.json')

# Fine-tune DETR/TATR
model = DetrForObjectDetection.from_pretrained(
    "microsoft/table-transformer-detection"
)
```

---

## Common Issues

### 1. Missing poppler

```bash
# Ubuntu/Debian
apt-get install poppler-utils

# macOS
brew install poppler

# Windows
# Download from https://github.com/oschwartz10612/poppler-windows
```

### 2. Empty Annotations

Ground truth may have empty line_items. The code handles this:

```python
if not valid_bboxes:
    return None  # Skip table annotation if no valid bboxes
```

### 3. Coordinate Systems

- Ground truth uses pixel coordinates (x, y, width, height)
- COCO format uses the same convention
- Both are normalized to 0-1000 scale for consistency

---

## Summary

This subtask implemented:

1. **PDF to Image conversion** using pdf2image/poppler
2. **Bounding box normalization** to 0-1000 scale
3. **COCO format generation** with images and annotations
4. **Table and row extraction** from ground truth
5. **Full integration** with format_tatr() function

The output is ready for training TATR or similar object detection models.
