# T003.1 - Dataset Preparation Script - Learning Guide

**Subtask**: 3.1 - Create prepare_datasets.py script
**Date**: 2025-12-04
**Audience**: Developers preparing data for ML model training

---

## What Was Built

A command-line script (`prepare_datasets.py`) that orchestrates the conversion of synthetic invoice data into formats suitable for training Hugging Face Transformer models.

---

## Why This Script?

### The Challenge

AI models require data in specific formats:
- **TATR (Table Transformer)**: Needs COCO format with bounding boxes
- **LayoutLMv3**: Needs BIO-tagged tokens with positions

Our synthetic data is in:
- PDF invoices
- JSON ground truth labels

### The Solution

A unified script that:
1. Reads synthetic PDFs and labels
2. Converts to model-specific formats
3. Creates train/val/test splits
4. Validates compatibility with Hugging Face

---

## Script Architecture

```
prepare_datasets.py
│
├── CLI Layer (argparse)
│   └── --input-dir, --output-dir, --format, --seed
│
├── Validation Layer
│   ├── validate_input_dir()
│   └── prepare_output_dir()
│
├── Format Layer (stubs for now)
│   ├── format_tatr()      → COCO format
│   └── format_layoutlm()  → BIO tokens
│
└── Main Entry Point
    └── Orchestrates the conversion
```

---

## Using the Script

### Basic Usage

```bash
# Convert to both formats (default)
python scripts/prepare_datasets.py

# Convert to TATR format only
python scripts/prepare_datasets.py --format tatr

# Convert to LayoutLM format only
python scripts/prepare_datasets.py --format layoutlm
```

### Full Options

```bash
python scripts/prepare_datasets.py \
    --input-dir data/synthetic \
    --output-dir data/formatted \
    --format both \
    --seed 42
```

### Short Options

```bash
python scripts/prepare_datasets.py -i data/synthetic -o data/formatted -f both -s 42
```

---

## Output Formats (Upcoming)

### TATR (COCO Format)

For table detection training:

```json
{
  "images": [
    {"id": 1, "file_name": "invoice_00001.png", "width": 612, "height": 792}
  ],
  "annotations": [
    {"id": 1, "image_id": 1, "category_id": 1, "bbox": [50, 100, 500, 400]}
  ],
  "categories": [
    {"id": 1, "name": "table"},
    {"id": 2, "name": "table_row"}
  ]
}
```

### LayoutLM (BIO Format)

For token classification training:

```json
{
  "tokens": ["RFC:", "XAXX010101ABC", "Total:", "$1,000.00"],
  "bboxes": [[10, 10, 50, 30], [60, 10, 150, 30], [10, 200, 50, 220], [60, 200, 150, 220]],
  "ner_tags": [0, 1, 0, 2],
  "image": "invoice_00001.png"
}
```

BIO Tags:
- `O` = Outside (not an entity)
- `B-RFC_EMISOR` = Beginning of RFC Emisor
- `I-RFC_EMISOR` = Inside RFC Emisor
- `B-TOTAL` = Beginning of Total
- etc.

---

## Design Decisions

### 1. Stub Functions

The `format_tatr()` and `format_layoutlm()` functions are stubs:

```python
def format_tatr(input_dir, output_dir, seed=42):
    # Create output directory
    tatr_output = os.path.join(output_dir, 'tatr')
    prepare_output_dir(tatr_output)

    # Stub: actual COCO conversion will be in subtask 3.2
    return {
        'status': 'stub',
        'processed': 0,
        'message': 'TATR formatting will be implemented in subtask 3.2'
    }
```

This allows:
- Testing the main script structure
- Incremental development
- Clear separation of concerns

### 2. Format Choices

Using `argparse.choices` for validation:

```python
parser.add_argument(
    '--format', '-f',
    type=str,
    choices=['tatr', 'layoutlm', 'both'],
    default='both',
    help='Output format'
)
```

Invalid formats are automatically rejected with helpful error messages.

### 3. Seed for Reproducibility

```python
parser.add_argument(
    '--seed', '-s',
    type=int,
    default=42,
    help='Random seed for reproducibility'
)
```

Ensures train/val/test splits are reproducible.

---

## Building CLI Tools with argparse

### Basic Pattern

```python
def create_argument_parser():
    parser = argparse.ArgumentParser(
        description='What the tool does',
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )

    parser.add_argument(
        '--long-name', '-s',  # Long and short options
        type=str,             # Type conversion
        default='value',      # Default value
        choices=['a', 'b'],   # Valid choices (optional)
        help='Description'    # Help text
    )

    return parser
```

### Usage in Main

```python
def main():
    parser = create_argument_parser()
    args = parser.parse_args()

    # Access arguments
    print(args.long_name)
    print(args.input_dir)

    return 0

if __name__ == '__main__':
    sys.exit(main())
```

---

## Testing CLI Scripts

### Mocking sys.argv

```python
from unittest.mock import patch

def test_main_with_args():
    with patch('sys.argv', ['script.py', '--format', 'tatr']):
        result = main()
        assert result == 0
```

### Mocking Functions

```python
def test_main_calls_formatter():
    with patch('sys.argv', ['script.py', '--format', 'tatr']):
        with patch('prepare_datasets.format_tatr') as mock_tatr:
            mock_tatr.return_value = {'status': 'ok'}
            main()
            mock_tatr.assert_called_once()
```

### Testing with Temp Directories

```python
import tempfile

def test_directory_creation():
    with tempfile.TemporaryDirectory() as tmpdir:
        output_dir = os.path.join(tmpdir, 'new', 'nested')
        prepare_output_dir(output_dir)
        assert os.path.isdir(output_dir)
```

---

## Directory Structure

### Input (from Task 2)

```
data/synthetic/
├── pdfs/
│   ├── invoice_00000.pdf
│   ├── invoice_00001.pdf
│   └── ...
└── labels/
    ├── invoice_00000.json
    ├── invoice_00001.json
    └── ...
```

### Output (to be created)

```
data/formatted/
├── tatr/
│   ├── train/
│   ├── val/
│   └── test/
└── layoutlm/
    ├── train/
    ├── val/
    └── test/
```

---

## Next Steps

### Subtask 3.2: TATR COCO Format
- Convert PDFs to images
- Extract table bounding boxes
- Create COCO annotations

### Subtask 3.3: LayoutLM BIO Format
- Run OCR on images
- Match tokens to ground truth
- Assign BIO tags

### Subtask 3.4: Train/Val/Test Splits
- 80% training, 10% validation, 10% test
- Use sklearn's train_test_split

### Subtask 3.5: Hugging Face Validation
- Test loading with datasets library
- Verify LayoutLMv3Processor compatibility

---

## Summary

This subtask established the foundation for data preparation:

1. **CLI Tool**: Clean interface with argparse
2. **Modular Design**: Separate functions for each format
3. **Stub Pattern**: Allows incremental development
4. **Full Test Coverage**: 31 tests validating all components

The script is ready to be extended with actual format conversion logic in subsequent subtasks.
