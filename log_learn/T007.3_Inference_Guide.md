# T007.3 Token Classification Inference - Learning Guide

## Overview
This guide explains how LayoutLMv3 performs token classification inference to label OCR words.

## The Inference Pipeline

```
OCR Words + Boxes + Image
        |
        v
    Processor
        |
        v
  [input_ids, attention_mask, bbox, pixel_values]
        |
        v
    LayoutLMv3
        |
        v
    Logits (per token)
        |
        v
   Argmax + Softmax
        |
        v
   Word ID Mapping
        |
        v
  Labeled Words
```

## Box Normalization

### Why Normalize to 0-1000?
LayoutLMv3 was trained with boxes in 0-1000 coordinate space:
```python
# Original (pixel coordinates)
box = (400, 300, 500, 400)  # On 800x600 image

# Normalized (0-1000 scale)
normalized = (
    400 * 1000 / 800,  # x1 = 500
    300 * 1000 / 600,  # y1 = 500
    500 * 1000 / 800,  # x2 = 625
    400 * 1000 / 600   # y2 = 666
)
```

### The Normalization Code
```python
width, height = image.size
normalized_boxes = [
    [
        int(box[0] * 1000 / width),
        int(box[1] * 1000 / height),
        int(box[2] * 1000 / width),
        int(box[3] * 1000 / height)
    ]
    for box in boxes
]
```

## Subword Tokenization

### The Problem
LayoutLMv3 uses subword tokenization (like BERT):
```
"ABC123456789" -> ["ABC", "##123", "##456", "##789"]
```

Each subword gets a prediction, but we want one label per word.

### word_ids() Solution
The processor provides `word_ids()` mapping:
```python
words = ["Hello", "ABC123456789", "World"]
# After tokenization:
# [CLS, Hello, ABC, ##123, ##456, ##789, World, SEP, PAD, ...]
# word_ids:
# [None, 0,     1,   1,     1,     1,     2,     None, None, ...]
```

### Mapping Back to Words
```python
word_ids = encoding.word_ids()
prev_word_id = None

for idx, word_id in enumerate(word_ids):
    # Skip special tokens (CLS, SEP, PAD)
    if word_id is None:
        continue

    # Skip continuation tokens (already saw this word)
    if word_id == prev_word_id:
        continue

    # This is the first token of a new word
    results.append({
        'word': words[word_id],
        'label': id2label[predictions[idx]],
        'confidence': probs[idx],
        'bbox': boxes[word_id]
    })
    prev_word_id = word_id
```

## Inference with no_grad

### Why no_grad()?
During inference, we don't need gradients:
```python
with torch.no_grad():
    outputs = self.model(**encoding)
```

Benefits:
- Lower memory usage (no gradient storage)
- Faster computation
- Prevents accidental training

## Getting Predictions and Confidence

### Predictions (argmax)
```python
# outputs.logits shape: (batch, seq_len, num_labels)
# Take highest-scoring label
predictions = outputs.logits.argmax(-1).squeeze().tolist()
# Result: [0, 1, 1, 1, 1, 0, 0, ...]  (label indices)
```

### Confidence (softmax)
```python
probs = torch.softmax(outputs.logits, dim=-1)  # Convert to probabilities
probs = probs.max(-1).values  # Take max probability
probs = probs.squeeze().tolist()
# Result: [0.95, 0.87, 0.89, ...]  (confidence scores)
```

## Common Gotcha: Dict Conversion

### The Bug
```python
encoding = self.processor(image, words, boxes=...)
encoding = {k: v.to(device) for k, v in encoding.items()}  # Now a dict!
word_ids = encoding.word_ids()  # ERROR: dict has no word_ids()
```

### The Fix
```python
encoding = self.processor(image, words, boxes=...)
word_ids = encoding.word_ids()  # Get BEFORE conversion
encoding = {k: v.to(device) for k, v in encoding.items()}
```

## Result Format

```python
[
    {
        'word': 'ABC123456789',
        'label': 'B-RFC_EMISOR',
        'confidence': 0.95,
        'bbox': (100, 50, 200, 70)
    },
    {
        'word': 'FECHA:',
        'label': 'O',
        'confidence': 0.88,
        'bbox': (50, 100, 120, 120)
    },
    {
        'word': '01/01/2024',
        'label': 'B-DATE',
        'confidence': 0.92,
        'bbox': (130, 100, 220, 120)
    }
]
```

## Key Takeaways

1. **Normalize boxes** to 0-1000 scale
2. **Get word_ids() early** before dict conversion
3. **Use no_grad()** for inference
4. **Handle subwords** by taking first token per word
5. **Skip special tokens** (None in word_ids)
