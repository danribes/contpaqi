# T007.3 Token Classification Inference - Implementation Log

## Subtask Overview
**Task**: 7 - LayoutLM Model Integration
**Subtask**: 7.3 - Implement token classification inference
**Status**: Completed
**Date**: 2025-12-07
**Tests**: 18 passing

## Description
Implemented the `predict()` method for running token classification inference on OCR words using LayoutLMv3.

## Files Created/Modified

| File | Description |
|------|-------------|
| `mcp-container/src/models/layoutlm.py` | Added predict() method |
| `tests/test_task007_3_layoutlm_inference.py` | 18 tests for inference |

## Implementation Details

### predict() Method Signature
```python
def predict(
    self,
    image: Any,
    words: List[str],
    boxes: List[Tuple]
) -> List[Dict]:
```

### Box Normalization
LayoutLMv3 expects boxes in 0-1000 scale:
```python
width, height = image.size
normalized_boxes = [
    [
        int(box[0] * 1000 / width),
        int(box[1] * 1000 / height),
        int(box[2] * 1000 / width),
        int(box[3] * 1000 / height)
    ]
    for box in boxes
]
```

### Processor Encoding
```python
encoding = self.processor(
    image,
    words,
    boxes=normalized_boxes,
    return_tensors="pt",
    truncation=True,
    padding="max_length",
    max_length=512
)

# Get word_ids before converting to dict (dict loses method)
word_ids = encoding.word_ids()

# Move tensors to device
encoding = {k: v.to(self.device) for k, v in encoding.items()}
```

### Inference with no_grad
```python
with torch.no_grad():
    outputs = self.model(**encoding)

predictions = outputs.logits.argmax(-1).squeeze().tolist()
probs = torch.softmax(outputs.logits, dim=-1).max(-1).values.squeeze().tolist()
```

### Word ID Mapping (Subword Handling)
```python
results = []
prev_word_id = None

for idx, word_id in enumerate(word_ids):
    if word_id is None or word_id == prev_word_id:
        continue
    if word_id >= len(words):
        continue

    results.append({
        'word': words[word_id],
        'label': self.id2label[predictions[idx]],
        'confidence': probs[idx],
        'bbox': boxes[word_id]
    })
    prev_word_id = word_id
```

## Return Format
```python
[
    {'word': 'ABC123456789', 'label': 'B-RFC_EMISOR', 'confidence': 0.95, 'bbox': (100, 50, 200, 70)},
    {'word': '$1,000.00', 'label': 'B-TOTAL', 'confidence': 0.92, 'bbox': (400, 300, 500, 320)},
]
```

## Key Design Decisions

1. **word_ids() before dict conversion**: The encoding object has a `word_ids()` method that's lost when converting to dict for device transfer
2. **Skip duplicate word_ids**: Handles subword tokenization by only taking first token of each word
3. **Skip None word_ids**: These are special tokens (CLS, SEP, PAD)
4. **Softmax for confidence**: More interpretable than raw logits

## Test Summary

| Test Class | Tests | Description |
|------------|-------|-------------|
| TestPredictMethodBasic | 2 | Returns list, empty without deps |
| TestPredictMethodSignature | 3 | Accepts image, words, boxes |
| TestBoxNormalization | 1 | Normalizes to 0-1000 scale |
| TestPredictWithMocks | 2 | Calls processor, uses no_grad |
| TestPredictResultStructure | 4 | Dict has word, label, bbox, confidence |
| TestWordIdMapping | 2 | Handles None and duplicates |
| TestProcessorParameters | 1 | Uses return_tensors="pt" |
| TestConfidenceScores | 1 | Includes confidence |
| TestLabelMapping | 1 | Maps ids to label strings |
| TestEmptyInputHandling | 1 | Handles empty words |
| **Total** | **18** | All passing |
